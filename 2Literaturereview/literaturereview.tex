\section{LITERATURE REVIEW}

The evolution of facial manipulation techniques, including Face Morphing, Face Swapping, Attribute Manipulation, and Deepfakes, poses a formidable challenge, particularly due to their potential to deceive and propagate misinformation [4]. The development of deepfakes, fueled by sophisticated technologies such as Generative Adversarial Networks (GANs), underscores the pressing need for robust detection mechanisms [5]. The exponential growth in deepfake technology magnifies the associated risks related to misinformation, deception, and breaches of privacy [6].\\

The literature introduces the Vision Transformer (ViT) as a significant breakthrough in computer vision, initially gaining prominence in natural language processing in 2017 with the publication of "Attention Is All You Need"[8]. While traditional applications in computer vision involved integrating attention with convolutional networks, recent studies underscore ViT's application in fundamental tasks such as classification, detection, and segmentation [9]. ViT's merits lie in its capacity to process entire images, adaptability to diverse image sizes, and effective training on extensive datasets, showcasing notable advancements over conventional models like CNNs and RNNs [10].\\

As DeepFakes become more widespread, there is a concerted effort to create classifiers using convolutional neural networks (CNN) to identify them. However, CNNs face issues like overfitting and an inability to grasp the correlation between local regions as a comprehensive feature of an image, which can lead to misclassification. The applicability of Vision Transformer (ViT) for deepfake detection over Generative Adversarial Networks (GANs) has been highlighted by recent explorations and developments. ViT's customized architecture for image analysis applications, in particular classification, helps identify abnormalities in images, notably those produced by models like GANs. When compared to traditional models, ViT's capacity for parallel processing, effective attention mechanisms, and transfer learning competency make it a reliable option for deepfake detection [9]. ViT is superior to Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) in image detection, but it also outperforms them because of its interpretability, parallel processing efficiency, and lower assumptions on sequence structure [10, 11].\\

In summary, the Vision Transformer emerges as a standout contender in the current spectrum of deepfake detection methodologies. Its distinctive features, including parallel processing, transfer learning proficiency, and efficient attention mechanisms, render it a compelling choice for our project. The Vision Transformer (ViT) sets itself apart from Convolutional Neural Networks (CNN) by employing self-attention mechanisms instead of traditional convolutional layers. ViT processes entire images simultaneously by dividing them into fixed-size patches, capturing global relationships effectively. This approach allows ViT to excel in tasks like image classification and detection, showcasing its adaptability and performance compared to CNNs.
