\section{LITERATURE REVIEW}

The evolution of facial manipulation techniques, including Face Morphing, Face Swapping, Attribute Manipulation, and Deepfakes, poses a formidable challenge, particularly due to their potential to deceive and propagate misinformation [4]. The development of deepfakes, fueled by sophisticated technologies such as Generative Adversarial Networks (GANs), underscores the pressing need for robust detection mechanisms [5]. The exponential growth in deepfake technology magnifies the associated risks related to misinformation, deception, and breaches of privacy [6].\\

In a study conducted at the University of California, researchers aimed to distinguish real people from fake ones using a Support Vector Machine (SVM) classifier. This classifier looked at facial expressions and changes in people's movements over time as key factors. They used an open-source tool called Open Face to analyze facial behaviors, capturing details like head movements and expressions from videos. While effective in certain situations, it's essential to note that this approach faced challenges when dealing with a wider range of datasets. This points out the need for improvements to make the model more versatile and better at detecting fake content across different datasets.[7]\\

The Vision Transformer (ViT) was introduced as a significant breakthrough in computer vision, initially gaining prominence in natural language processing in 2017 with the publication of "Attention Is All You Need"[8]. While traditional applications in computer vision involved integrating attention with convolutional networks, recent studies underscore ViT's application in fundamental tasks such as classification, detection, and segmentation [9]. ViT's merits lie in its capacity to process entire images, adaptability to diverse image sizes, and effective training on extensive datasets, showcasing notable advancements over conventional models like CNNs and RNNs [10].\\

As DeepFakes become more widespread, there was a concerted effort to create classifiers using convolutional neural networks (CNN) to identify them. However, CNNs face issues like overfitting and an inability to grasp the correlation between local regions as a comprehensive feature of an image, which can lead to misclassification. The applicability of Vision Transformer (ViT) for deepfake detection over Generative Adversarial Networks (GANs) has been highlighted by recent explorations and developments. ViT's customized architecture for image analysis applications, in particular classification, helps identify abnormalities in images, notably those produced by models like GANs. When compared to traditional models, ViT's capacity for parallel processing, effective attention mechanisms, and transfer learning competency make it a reliable option for deepfake detection [9]. ViT is superior to Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) in image detection, but it also outperforms them because of its interpretability, parallel processing efficiency, and lower assumptions on sequence structure [10, 11].\\

In the paper published by Lorenzo Papa, Paolo Russo and Luping Zhou, it mentions about Vision Transformer (ViT) architectures working better for real-world computer vision tasks. Even though ViTs are good at getting global information, they face challenges as they get bigger and need more computing power, especially for high-resolution images. The paper explores ways to make ViTs more efficient, like using smaller architectures, pruning, knowledge distillation, and quantization. They introduce a new metric, the Efficient Error Rate, to compare how well models work on different hardware. By using math and analysis, the paper aims to give insights into making ViTs better for all sorts of applications, and it ends by talking about the challenges and possible future research.

In summary, while the other architecture stands out on their respective scenarios, the Vision Transformer emerges as a standout contender in the current spectrum of deepfake detection methodologies. Its distinctive features, including parallel processing, transfer learning proficiency, and efficient attention mechanisms, render it a compelling choice for our project. The Vision Transformer (ViT) sets itself apart from Convolutional Neural Networks (CNN) by employing self-attention mechanisms instead of traditional convolutional layers. ViT processes entire images simultaneously by dividing them into fixed-size patches, capturing global relationships effectively. This approach allows ViT to excel in tasks like image classification and detection, showcasing its adaptability and performance compared to CNNs.
