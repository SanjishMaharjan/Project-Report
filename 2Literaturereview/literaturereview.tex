\section{LITERATURE REVIEW}

The facial manipulation is a techniques of altering someone's face in images or videos, often done using computer software. There are different types of facial manipulation techniques, each serving different purposes such as Face Morphing where two or more faces are blended together and create a seamless transition between them, Face Swapping, which involves replacing one person's face with another's while keeping the rest of the image intact, Attribute manipulation, where different attributes of face like nose, eyes, etc. are manipulated and Deepfakes, which uses advanced artificial intelligence and machine learning to create highly realistic digital contents by superimposing one person's face onto another person's body [11]. These videos can convincingly mimic speech, expressions, and gestures, making them a source of concern for potential misinformation and privacy issues.
\\\\
The generation of deepfakes has had a rapid growth. Generating deepfakes involves using advanced technology to create fake videos or images that look very realistic. The deepfakes are created using deep learning techniques like Generative Adversarial Networks (GANs). The GAN technique involve two main components: a generator and a discriminator. The generator takes random noise as input and produces data, like images. The discriminator evaluates this generated data alongside real data and tries to tell them apart. As training progresses, the generator refines its output to become more convincing, while the discriminator becomes better at distinguishing real from fake. This competitive process pushes the generator to create increasingly realistic data that can be mistaken for real. As training progresses, the generator refines its output to become more convincing, while the discriminator becomes better at distinguishing real from fake. This competitive process pushes the generator to create increasingly realistic data that can be mistaken for real [9]. 
\\\\
This remarkable advancement in deepfake technology underlines the desperate need for effective deepfake detection mechanisms. As these deceptively authentic videos, images, and audio recordings becomes more widespread, the risk of misinformation, deception, and breaches of privacy escalates hugely [10]. Detecting deepfakes is a difficult challenge. However, researchers and experts are actively exploring diverse strategies to differentiate between these genuine and manipulated content. These strategies often involve detecting facial features for anomalies, identifying patterns and residuals in lighting and shadows, and uncovering distortions introduced during the manipulation process such as iris and pupil deformation [12].
\\\\
Since 2017, the Transformer has been very renowned as a new type of neural architecture for natural Language Processing which encodes the given input data into a powerful feature, with the help of attention mechanism.  With the research article published in 2017, named "Attention Is All You Need" [6], by taking it as a framework, numerous researches has been done recently upon the visions task as well [3]. This is where the Visual Transformer comes in. 
\\\\
While the Transformer Architecture has been a new standard for Natural Language Processing tasks, its application to computer vision remains limited. From many recent studies and researches done, the common points mentioned was that in vision, either the attention is applied in conjunction with convolution's networks, or used to replace certain components of convolutional network while keeping their overall structure in place [4].
The Transformer-liked architecture has been employed in the computer vision field in present context in three fundamental computer vision tasks, namely classification, detection and segmentation [2]. Our work lies within one of these fundamental tasks i.e the detection of artificially generated images of a human face.
\\\\
 The Visual Transformer has been gaining many contributions in recent years such that its capabilities has increased beyond the old traditional models. As a matter of fact, the Visual Transformer model has clear advantages over traditional models such as CNNs and RNNs in specific situations. It can process whole images in sections, which helps it understand the bigger picture. Also, it is good with working on different image sizes and tasks. 
 Training these models on big datasets first and then adjusting it for specific tasks makes it really flexible compared to the traditional model [5]. One of its key characteristics, self-attention, not only helps us understand how the model works but also reminds us to pick the right model based on the job, data, and what we have.
\\\\
In the Vision Transformer, the use of multi-headed self attention is used which allows the model to associate each individual testing attributes of input to run parallelly. The query, key and values vectors are used as the calculation attributes to calculate scores, ultimately gaining the softmax score for obtaining probability values and scaled scores for attention weights. These multi-headed attention are the concatenated for further processes. Before feeding the inputs in the ViT model, the input datas are to be preprocessed first. Initially, the frame extraction process is done, followed by the features abstraction. The features such as facial structure can be extracted with the help of Face Alignment Library. Then the input is normalized followed by resizing and patching, which provides a representation of NLP for the input to Vision Transformer [4].
\\\\
From recent researches done Vision Transformer (ViT) is deemed more suitable than Generative Adversarial Networks (GANs) for deepfake detection because Vision Transformer is specifically designed for image analysis tasks like classification. GANs are like artists that make fake pictures, but they're not as good at spotting fakes. Also, the tricks that make GANs create fakes can also fool themselves when looking for fakes. Vision transformer way of looking at pictures makes it a good choice for catching fake ones, because it's good at seeing small things that don't match up. Also, Vision Transformers (ViTs) have advantages over Recurrent Neural Networks (RNNs) in computer vision due to their parallel processing, better handling of long-range dependencies, efficient attention mechanisms, fewer assumptions about sequence structure, transfer learning capabilities, reduced overfitting, and interpretable representations [2]. However, RNNs remain strong for sequential data tasks and short-term dependencies[13]. Whereas against Convolutional Neural Networks (CNNs), ViT has advantages of efficient parallel processing, reduced invariance assumption, scalability, interpretable representations, fewer specialized architectures, and transfer learning. But CNNs is better for tasks requiring local features and spatial hierarchies [13].
\\\\
In summary, with all these distinct attributes and merits the Vision Transformer imposes itself significantly compared to others at present time and with understanding of the factors more compatible for our project, we have selected the Vision Transformer as the major model for our project development.

