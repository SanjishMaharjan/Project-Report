\subsection{Working Description}

The comprehensive and detailed explanation of the functioning of our model is provided below:\\

Before feeding an image into the model, it undergoes essential preprocessing to ensure compatibility. This includes converting the image into numeric vectors, a step known as input embedding. This process is crucial for aligning raw image data with the Vision Transformer (ViT) architecture. The image is divided into non-overlapping patches, each undergoing bilinear embedding to create fixed-size vectors. This transforms the image into a sequential arrangement of vectors, enabling sequential processing by the transformer model.\\

Initially, the input image undergoes segmentation into patches, a necessary step to conform with the sequential processing design of transformers. The formulation of these patches considers factors such as the image's height, width, color channels (red, green, and blue), and the batch size for multiple images. The input image, characterized by specific dimensions in terms of height and width, three color channels, and an optional batch dimension for multiple images, is thus transformed into a sequence to align with the architecture of transformers.\\
\begin{figure}[htbp]
    \centering
    \includegraphics[width=4in]{img/colorbatch.png}
    \caption{\textit{Image with RGB color channels}}
\end{figure}\\
The images undergo resizing and conversion into tensors, representing the initial phase of the process. Following this, the "PatchEmbedding" module is employed to reshape the tensors. This reshaping step plays a pivotal role, setting the stage for a bilinear transformation that ultimately yields embedding vectors. A bilinear transformation was utilized on flattened image patches of one-dimensional sequence, to map them to a desired dimension.\\

Einops-Einstein Operation, is a Python library that was used which simplifies tensor operations. Inspired by Einstein summation conventions, it offers an expressive syntax for reshaping and reducing and manipulation of tensors. \\

After completing the patching process, the original positions of the patches within an image becomes unknown. To address this, position embedding is employed. Since the attention mechanism in transformers is position-independent, position embedding is introduced to enable the model to comprehend the specific locations of each patch in the original image. A vector is assigned to each patch, representing its unique position within the image. The values within these position embedding vectors are derived from sine and cosine functions.\\

The positional encoding can be obtained as:\\
\\
\[
    PE_{(pos, 2i)} = \sin\left(\frac{pos}{{10000}^{(2i/d)}}\right)
\]

\[
    PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{{10000}^{(2i/d)}}\right)
\]
\\
Where:
\begin{align*}
    \text{pos} & : \text{the position of the token in the sequence.} \\
    i          & : \text{the dimension of the positional encoding.}  \\
    d          & : \text{the dimension of the model or embedding.}
\end{align*}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=4in]{img/plot for sine and cosine wave.png}
    \caption{\textit{Value vs Position graph for sine and cosine wave}}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=4in]{img/pos 10.png}
    \caption{\textit{Graph for sine and cosine wave  at position 10}}
\end{figure}
For a patch at position 10, the values in its associated position embedding vectors is equivalent to to corresponding values intersecting within the curve as shown in the graph.\\

In the process of adding position embeddings to an image, we introduced a special vector known as the CLS token, where CLS stands for Classification Token. The CLS token is a unique vector assigned to each input image. Initially, it receives random values and is considered a placeholder. However, as the model learns, these values are adjusted based on the information gathered from all other input patches. This adjustment process makes the CLS token a "learnable vector," as it evolves during training to contribute valuable information for the classification task.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=5in]{img/CLS token.png}
    \caption{\textit{CLS Token and position embedding vectors}}
\end{figure}

The CLS token functions as a universal feature extractor, capturing the essence of the entire image. This extracted information can then be utilized for various tasks downstream. The learning positional embeddings helps reduce inductive biases. These embeddings are added on top of input embedding vectors and are not concatenated with them.\\
\\
The Embedded Patches are then passed onto the Encoder section of the model.\\
\begin{figure}[htbp]
    \centering
    \includegraphics[width=6.3in]{img/transformer_encoder.png}
    \caption{\textit{Transformer Encoder}}
\end{figure}\\

Multihead attention is a mechanism designed to capture different aspects or relationships within input sequences. It involves splitting the input into multiple heads or sets, each with its own set of learned weights. The attention mechanism is then applied independently to each head in parallel.
The multihead attention is the scaled dot product attention mechanism of the transformer and allows to share info between different inputs.\\

In multihead attention, there are three primary inputs:\\

\textbf{Query (Q):} This represents the set of queries used to retrieve information from the input sequence. It is usually derived from the input data and is transformed to capture relevant features.\\

\textbf{Key (K):} The key input consists of keys associated with each element in the input sequence. Like the query, it is derived from the input data and aims to provide information about the relationships between different elements.\\

\textbf{Value (V):} The value input includes the values or content associated with each element in the input sequence. It also comes from the input data and provides the actual information that will be weighted and combined based on the attention mechanism.\\

The Multihead attention process is also regarded as the process where there is inter linkage between the inputs such that exchange of information takes place. To put these Q, K and V to a simple understanding, value is the component that actually is communicated in multihead attention process, query is equivalent for "what i am looking for" and key is equivalent for "what i have."\\

For further explanation  on Multihead Attention, suppose we have inputs to attention blocks of N embeddings (total N+1 including CLS token) and dimension D. With the help of N and D, a stacked matrix can be obtained.
\[ X \in \mathbb{R}^{N \times D}\]

For a single head attention we project each patch embeddings for 3 separate iteration to produce the key, queries and values. The three matrices to produce the keys, queries and values would be:
\[ W^Q \in \mathbb{R}^{D\times d_k}\]
\[ W^K \in \mathbb{R}^{D \times d_k}\]
\[ W^V \in \mathbb{R}^{D \times d_v}\]
Where:
\begin{align*}
    d_k & : \text{the dimension of query and key .} \\
    d_v & : \text{the dimension of values.}
\end{align*}
Therefore, the key(K), query(Q) and value(V) can be obtained when the original embedding matrix X is multiplied with the above mentioned matrices.
\[Q = X.W^Q \in \mathbb{R}^{N\times d_k}\]
\[K = X.W^K \in \mathbb{R}^{N\times d_k}\]
\[V = X.W^V \in \mathbb{R}^{N\times d_v}\] \\

The multihead attention will contain 'h' numbers of heads, each head representing as a single. All these multiple heads undergo computation in parallel. \\

The general form of  scaled dot-product attention is given as:
\[\text{Attention}(h) = \text{softmax}\left(\frac{Q_h * K_h ^T}{\sqrt{d_k}}\right) V_h\]
The softmax is applied along the rows of the matrix to normalize them to probability vectors and $d_k$ helps in avoiding peaky affinities.When attention distributions are too peaky, it suggests the model is assigning a significantly higher weight to a small subset of elements in the input sequence while largely ignoring the rest.\\

Finally, \[\text{Multihead Attention} = \text{concat} (h_1, h_2, h_3,...,H)* W^o \]

$W^0$ is added to ensure the right dimension is obtained.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=5in]{img/Attentionfig.png}
    \caption{\textit{Attention head visualization}}
\end{figure}

The asymptotic complexity, often denoted as Big O notation, for Multi-Head Attention (MHA) in the Transformer model is typically expressed as\[ O(N \cdot d^2 \cdot H)\], where:
\begin{itemize}
    \item $N$ is the sequence length,
    \item $d$ is the dimensionality of the model's hidden representations,
    \item $H$ is the number of attention heads.
\end{itemize}
Because of this when processing long sequences, the computational cost of Multihead Attention, can be substantial.\\

At present context, multihead attention doesn't require building from scratch; instead, it can be accessed through a pre-existing module.\\

After the MultiHead Attention layer, an essential component is the Multi-Layer Perceptron (MLP). The MLP comprises two bilinear layers, incorporating the GeLU activation function and followed by a Dropout layer. Unlike in Multihead Attention, where components interact with each other, in MLP, the inputs are treated independently and don't communicate with each other directly.\\

The expression for MLP is given as:\\

\[\text{MLP}(x) = W_2 \cdot \sigma(W_1 \cdot x + b_1) + b_2\]
where:
\begin{align*}
    x      & : \text{the input vector,}                                \\
    W_1    & : \text{the weight matrix for the first bilinear layer,}  \\
    b_1    & : \text{the bias vector for the first layer,}             \\
    \sigma & : \text{GeLU activation function (non-linear),}           \\
    W_2    & : \text{the weight matrix for the second bilinear layer,} \\
    b_2    & : \text{the bias vector for the second layer.}
\end{align*}
Usually, an expansion factor of 4 is commonly employed as it is observed to be effective based on empirical evidence.\\
\\

\begin{figure}[htbp]
    \centering
    \includegraphics[width=5in]{img/MLP workings.png}
    \caption{\textit{Workings in MLP}}
\end{figure}
\newpage
\begin{figure}[htbp]
    \centering
    \includegraphics[width=2in]{img/residual connection.png}
    \caption{\textit{Residual connection}}
\end{figure}
The Residual connection, also known as Skip connection, addresses the vanishing gradient issue by facilitating smoother gradient flow throughout the network during back-propagation. By incorporating a shortcut that maintains an identity mapping, residual connections empower the model to learn residual features, capturing the distinctions between the input and the output of the layer. In this procedure, the respective inputs are applied back into the result from obtained form MLP and MHA.\\

The general form can be represented as:\\

For Multihead Output\\
\[(Y) = (X)+\text{MHA (Normalization (X))}\]

For MLP Output
\[(OUTPUT) = (Y)+\text{MLP (Normalization (Y))}\]
where:
\begin{align*}
    \text{X} & : \text{Input for MHA}                 \\
    \text{Y} & : \text{Output from MHA Input for MLA} \\
\end{align*}

Lastly, Layer Normalization, provided by Keras through the LayerNormalization module, is used for the normalization. Its integration helps stabilize training and maintain proper feature scaling, contributing to improved performance in image recognition tasks. It provides multiple benefits, such as enhanced training stability, reduction of internal covariate shifts, and improved generalization. By ensuring uniform activation distributions across various layers, it facilitates smoother optimization, leading to an overall more robust and efficient model.


\subsubsection{Frontend Mobile App Development}

\begin{itemize}
    \item\textbf{Overview}:\\
    Our frontend is designed to provide a user-friendly interface for interacting with the Deepfake Detection App using React Native. It includes features such as user authentication, profile management, and image testing for deepfake detection.

    \item\textbf{Components}:\\
    The frontend consists of several components:

    \begin{itemize}
        \item \textbf{Login Screen}: The login screen serves as the entry point for users to access the application. It presents a clean and intuitive interface where users can input their credentials to authenticate and gain access to their account.

        \item \textbf{Signup Screen}: The signup screen facilitates the registration process through a form where users can input their personal details such as name, email address, and desired password. Upon successful completion of the signup process, users are usually redirected to the login screen to authenticate and access their newly created account.

        \item \textbf{Media Testing Screen}: The media testing screen provides functionality for users to analyze both images and videos for the presence of deepfakes or other forms of manipulation. It offers a straightforward interface for users to upload media files from their device or capture new ones using the device's camera. After uploading a file, the screen initiates a deepfake detection process, using trained modeled to analyze the media and determine its authenticity. The result of the analysis is then displayed to the user, indicating whether the media is likely to be genuine or altered.

        \item \textbf{Profile Screen}: The profile screen provides users with a personalized view of their account information.

        \item \textbf{History Screen}: The history screen allows users to view a chronological record of their past activities within the application. This include a history of logged-in sessions, image testing results. Each entry in the history list includes details such as the date and time of the activity, along with results.
    \end{itemize}

    \item \textbf{Styling}:\\
          The frontend of our application is styled to ensure a seamless user experience, blending custom-styled components with predefined elements from React Native. Custom-styled components are utilized to maintain a cohesive visual identity and streamline the design process, promoting consistency and efficiency.

          \item\textbf{Navigation}:\\
          We have implemented navigation between screens using the React Navigation library. This allows users to navigate seamlessly between different sections of the app.
\end{itemize}

\subsection{Backend API Development}

We have used FastAPI to build backend API, providing robust and efficient routing for handling requests and deploying machine learning models trained with Keras for deepfake detection.

\begin{itemize}
    \item \textbf{Overview}:\\
          The backend API serves as the foundation for our Deepfake Detection App, handling various endpoints for user authentication, media testing, and model deployment. It integrates seamlessly with the frontend to provide a cohesive user experience.

    \item \textbf{Endpoints}:\\
          The backend API consists of the following endpoints:

          \begin{itemize}

              \item \textbf{Authentication Endpoint}\\
                    The authentication endpoint provides a secure mechanism for users to log in and access their accounts. Upon receiving login credentials, the endpoint verifies the provided username and password by comparing them with the hashed credentials stored in the database. To enhance security, the passwords are hashed using the bcrypt hashing algorithm before being stored in the database. If the credentials match, the endpoint generates a JSON Web Token (JWT) containing user information and a unique identifier, which is signed using a secret key. This token is then returned to the client and can be used for subsequent authorized access to protected resources without the need to repeatedly provide credentials.

                    \begin{figure}[htbp]
                        \centering
                        \includegraphics[width=5in]{img/JSON-Web-Tokens-01.png}
                        \caption{\textit{JWT Workflow}}
                    \end{figure}

              \item \textbf{Signup Endpoint}:\\
                    The signup endpoint facilitates user registration by processing signup requests and creating new user accounts in the database. The password is securely hashed using the bcrypt algorithm before being stored in the database to protect user privacy. After successful validation and hashing, the endpoint creates a new user account with the provided details and assigns a unique identifier.

                    \begin{figure}[htbp]
                        \centering
                        \includegraphics[width=5in]{img/example-of-hashing-during-signup.png}
                        \caption{\textit{Hashing}}
                    \end{figure}
                    \newpage
              \item \textbf{Media Testing Endpoint}:\\
                    This endpoint is responsible for analyzing images and videos uploaded by users to detect the presence of deepfakes. It utilizes machine learning models deployed with Keras to perform deepfake detection and returns the analysis results to the frontend for display.

              \item \textbf{Profile Endpoint}:\\
                    The profile endpoint handles requests related to user profiles, allowing users to view their account information.

              \item \textbf{History Endpoint}: The history endpoint manages user activity logs, maintaining a record of past interactions within the application. It stores information such as login sessions, media testing results, and other relevant data for reference and analysis.
          \end{itemize}

    \item \textbf{Routing and Middleware}:\\
          FastAPI's routing system facilitates the organization and management of various endpoints within our backend application. Through intuitive decorators and path operations, we define the routes that handle incoming HTTP requests and specify the corresponding logic to execute for each request type (GET, POST, PUT, DELETE, etc.).

          Additionally, middleware functions play a crucial role in enhancing the capabilities and security of our backend infrastructure. These middleware components intercept incoming requests and responses, allowing us to implement cross-cutting concerns such as authentication, logging, request validation, and error handling.

          \begin{figure}[htbp]
              \centering
              \includegraphics[width=6in]{img/Frontend_Backend.png}
              \caption{\textit{System Workflow Architecture}}
          \end{figure}

    \item \textbf{Model Deployment}:\\
          Our backend API integrates machine learning models trained with Keras for deepfake detection. These models are deployed as endpoints within the API, allowing them to process incoming media files and generate predictions on-the-fly. Model deployment is handled efficiently to ensure optimal performance and scalability.

\end{itemize}


\newpage