\subsection{Loss function }
The loss function employed in the Vision Transformer (ViT) model is a critical component for training the network and optimizing its performance in image classification tasks. The ViT loss function is designed to measure the disparity between the predicted class probabilities and the ground truth labels.


\subsubsection{Cross-Entropy Loss}
The primary loss utilized in ViT is the Cross-Entropy Loss, also known as categorical cross-entropy. This loss is well-suited for classification problems, such as image classification. It quantifies the difference between the predicted probability distribution (produced by the softmax activation) and the true distribution of class labels.
\\
\\
Mathematically, the Cross-Entropy Loss for a single training example is defined as:
\\

\[
L(y, \hat{y}) = -\sum_i y_i \cdot \log(\hat{y}_i)
\]

\text{where:}
\begin{align*}
y & \text{ is the ground truth label vector,} \\
\hat{y} & \text{ is the predicted probability distribution vector,} \\
i & \text{ iterates over all classes.}
\end{align*}
\\
Cross-Entropy Loss is preferred for image classification, including with Vision Transformers, due to its alignment with classification objectives, compatibility with softmax activation, and effective handling of probability distributions. Its use encourages accurate and calibrated predictions during training.
